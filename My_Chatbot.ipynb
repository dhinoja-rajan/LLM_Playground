{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69388c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Standard libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import ipaddress\n",
    "\n",
    "# üß™ Third-party libraries\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# üåê Selenium (Web Automation)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# üß† AI/NLP\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# üîç Vector DB\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd323211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory where your scraped text files are saved\n",
    "data_dir = \"scraped_pages\"\n",
    "\n",
    "# This will store all your documents\n",
    "documents = []\n",
    "\n",
    "# Loop through all text files in that folder\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read().strip()\n",
    "            \n",
    "            # Split by lines\n",
    "            lines = raw_text.split(\"\\n\")\n",
    "            \n",
    "            # Extract URL from the first line\n",
    "            url = None\n",
    "            if lines and lines[0].startswith(\"URL:\"):\n",
    "                url = lines[0].replace(\"URL:\", \"\").strip()\n",
    "                content = \"\\n\".join(lines[1:]).strip()\n",
    "            else:\n",
    "                content = raw_text  # fallback if URL line not found\n",
    "            \n",
    "            # Save as structured dict\n",
    "            documents.append({\n",
    "                \"filename\": filename,\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9078187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0be750c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    split_chunks = splitter.split_text(doc[\"content\"])\n",
    "    for chunk in split_chunks:\n",
    "        chunks.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"source\": doc[\"filename\"],\n",
    "            \"url\": doc[\"url\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "144c3f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7339"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "773f3df3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7880a0b985ca4d198e7080c139846442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m texts_to_embed \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# ‚úÖ Batch encode all 7,339 chunks at once\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m vectors \u001b[38;5;241m=\u001b[39m embedding_model\u001b[38;5;241m.\u001b[39mencode(\n\u001b[0;32m      9\u001b[0m     texts_to_embed,\n\u001b[0;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,                 \u001b[38;5;66;03m# ‚ö° Increase if you have more RAM/GPU\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     show_progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m     normalize_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m      \u001b[38;5;66;03m# ‚úÖ Normalize for cosine similarity\u001b[39;00m\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# ‚úÖ Attach embeddings to your chunks\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, vec \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(vectors):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m-> 1052\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1053\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1054\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\SentenceTransformer.py:1133\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[1;34m(self, input, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m             module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[0;32m   1128\u001b[0m         module_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1129\u001b[0m             key: value\n\u001b[0;32m   1130\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mforward_kwargs)\n\u001b[0;32m   1132\u001b[0m         }\n\u001b[1;32m-> 1133\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[0;32m   1134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sentence_transformers\\models\\Transformer.py:437\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, features, **kwargs)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[0;32m    431\u001b[0m trans_features \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    432\u001b[0m     key: value\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    435\u001b[0m }\n\u001b[1;32m--> 437\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    438\u001b[0m token_embeddings \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m token_embeddings\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    990\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    994\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 996\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    997\u001b[0m     embedding_output,\n\u001b[0;32m    998\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m    999\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1000\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1001\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1002\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1003\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1004\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1005\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1006\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1007\u001b[0m )\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1009\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:651\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    648\u001b[0m layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    649\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 651\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    652\u001b[0m     hidden_states,\n\u001b[0;32m    653\u001b[0m     attention_mask,\n\u001b[0;32m    654\u001b[0m     layer_head_mask,\n\u001b[0;32m    655\u001b[0m     encoder_hidden_states,  \u001b[38;5;66;03m# as a positional argument for gradient checkpointing\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_attention_mask,\n\u001b[0;32m    657\u001b[0m     past_key_value\u001b[38;5;241m=\u001b[39mpast_key_value,\n\u001b[0;32m    658\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    659\u001b[0m )\n\u001b[0;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    662\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:553\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    543\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    552\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 553\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    554\u001b[0m         hidden_states,\n\u001b[0;32m    555\u001b[0m         attention_mask,\n\u001b[0;32m    556\u001b[0m         head_mask,\n\u001b[0;32m    557\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    558\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    559\u001b[0m     )\n\u001b[0;32m    560\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    562\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:483\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    475\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    481\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    482\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 483\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    484\u001b[0m         hidden_states,\n\u001b[0;32m    485\u001b[0m         attention_mask,\n\u001b[0;32m    486\u001b[0m         head_mask,\n\u001b[0;32m    487\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    488\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    489\u001b[0m         past_key_value,\n\u001b[0;32m    490\u001b[0m         output_attentions,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[0;32m    492\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    493\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\models\\bert\\modeling_bert.py:408\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    406\u001b[0m )\n\u001b[1;32m--> 408\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[0;32m    409\u001b[0m     query_layer,\n\u001b[0;32m    410\u001b[0m     key_layer,\n\u001b[0;32m    411\u001b[0m     value_layer,\n\u001b[0;32m    412\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m    413\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[0;32m    414\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[0;32m    415\u001b[0m )\n\u001b[0;32m    417\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    418\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ‚úÖ Load the model\n",
    "embedding_model = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
    "\n",
    "# ‚úÖ Prepare text with \"passage: \" prefix (required by E5)\n",
    "texts_to_embed = [f\"passage: {chunk['chunk']}\" for chunk in chunks]\n",
    "\n",
    "# ‚úÖ Batch encode all 7,339 chunks at once\n",
    "vectors = embedding_model.encode(\n",
    "    texts_to_embed,\n",
    "    batch_size=64,                 # ‚ö° Increase if you have more RAM/GPU\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True      # ‚úÖ Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "# ‚úÖ Attach embeddings to your chunks\n",
    "for i, vec in enumerate(vectors):\n",
    "    chunks[i][\"embedding\"] = vec.tolist()  # Optional: convert to list if saving to JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef8626df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "# if not load_dotenv():\n",
    "#     print(\"Warning: .env file not loaded. Make sure it exists.\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# Replace with your real values\n",
    "PINECONE_API_KEY = PINECONE_API_KEY\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"PINECONE_API_KEY environment variable not set\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Configuration\n",
    "index_name = \"chatbot-index\"\n",
    "dimension = 768  # for E5-base-v2\n",
    "metric = \"cosine\"\n",
    "\n",
    "# ‚úÖ Check if index exists, else create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",             # Based on Pinecone environment info\n",
    "            region=\"us-east-1\"       # your region from Pinecone Console\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ‚úÖ Connect to index\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d69a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7339/7339 [01:25<00:00, 85.72it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_size = 100  # Upsert in batches (efficient & safe)\n",
    "batch = []\n",
    "\n",
    "for i, chunk in enumerate(tqdm(chunks)):\n",
    "    vector_id = f\"chunk-{i}\"  # Unique ID for Pinecone\n",
    "\n",
    "    vector = {\n",
    "        \"id\": vector_id,\n",
    "        \"values\": chunk[\"embedding\"],  # The 768-dimensional vector from E5\n",
    "        \"metadata\": {\n",
    "            \"text\": chunk[\"chunk\"],     # Original text chunk\n",
    "            \"source\": chunk[\"source\"],  # File or page name\n",
    "            \"url\": chunk[\"url\"]         # Optional if you have it\n",
    "        }\n",
    "    }\n",
    "\n",
    "    batch.append(vector)\n",
    "\n",
    "    # ‚¨ÜÔ∏è Upload in batches\n",
    "    if len(batch) == batch_size or i == len(chunks) - 1:\n",
    "        index.upsert(vectors=batch)\n",
    "        batch = []  # Reset for next batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07cf9d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 768,\n",
      " 'index_fullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'': {'vector_count': 7339}},\n",
      " 'total_vector_count': 7339,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "# Check index statistics\n",
    "stats = index.describe_index_stats()\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59d20617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FetchResponse(namespace='', vectors={'chunk-0': Vector(id='chunk-0', values=[-0.0106099034, -0.00864486303, -0.0387433507, -0.043938525, 0.0633867756, -0.0390194878, 0.0066082906, 0.048261147, -0.0313188434, -0.0260302089, 0.0164223239, 0.0581865571, -0.0173711032, -0.00773743819, -0.0483874753, 0.0336582065, 0.0331584737, 0.022745844, 0.0498974845, -0.0338597298, -0.0186276268, -0.0377601311, 0.0167887714, 0.0185805745, -0.0166405775, 0.0276364014, 0.00794698764, 0.0644459426, -0.0754811838, -0.0448967293, 0.0353211276, 0.0438374244, -0.0116759185, -0.0616721176, -0.0451578908, 0.00647079479, -0.0518077575, -0.0151456399, -0.0379987471, -0.0182747412, 0.00688091805, -0.0344539, -0.0240712017, -0.0131164482, -0.0345140547, -0.0188526567, -0.0644629523, 0.0324888118, -0.0386358537, -0.0400037, -0.0221779328, 0.0447198711, 0.0410682708, 0.0119176675, -0.0240652524, 0.0281524453, -0.025440773, 0.0131647671, -0.035822086, -0.0175059903, 0.0604489222, 5.32358717e-05, 0.0549181849, 0.0242440868, 0.0454725064, 0.0368940458, 0.0136526339, 0.0452598594, -0.0110243, 0.0146996127, -0.0236528069, -0.0259901695, 0.00265769195, -0.0176806971, -0.0368614048, -0.0167822968, 0.0209221989, 0.00421201671, 0.0354063846, 0.0143330237, -0.0453991853, -0.00377352722, -0.000171953288, 0.0481866337, 0.0168724488, -0.0156277679, -0.0421854816, -0.0013064841, -0.0276392829, 0.0243241936, -0.0296653, -0.0270693246, 0.0516789071, 0.0219823234, 0.0376158133, -0.0365673415, 0.0581123345, -0.0310894474, 0.0334894136, 0.0149395615, -0.0544290803, -0.0253061689, -0.0411019474, 0.00920267589, -0.0338147581, -0.0424157642, 0.0252520051, 0.0242911298, -0.0180931967, 0.00807617512, -0.051901821, -0.0143233864, -0.0218779594, -0.0364620835, -0.0317709073, 0.0355359763, 0.065279685, -0.00472709769, -0.0471862741, 0.00112309481, -0.00222259783, 0.0445330888, 0.0441274382, 0.0290853363, 0.0280531514, -0.0108894706, 0.0264764298, 0.0186813567, 0.00214726105, -0.0193210971, -0.0107413409, 0.00682177115, 0.0487844534, -0.0257798657, 0.00294014602, -0.0133276498, -0.0563780814, -0.030073, -0.0038687638, -0.0323599763, 0.0222348943, 0.047858119, 0.0210432876, -0.0270411335, 0.0626368448, -0.0353962258, -0.0421908572, -0.0507920198, -0.0510150082, 0.0293048974, 0.0340168513, 0.0350148, 0.0348061137, -0.028034931, -0.0465085916, 0.0427498631, 0.0357216373, -0.0237480924, -0.0289283227, -0.0148123205, 0.0056919069, 0.0446370617, 0.02787094, 0.018707592, 0.00659749424, -0.0247061346, -0.0320885368, -0.06591364, 0.0350051038, 0.0642161593, -0.0310373027, -0.0372084975, 0.0610546283, -0.0542932861, 0.00394225912, 0.0594446398, 0.0302485116, 0.0216147564, -0.0207333, -0.0050931056, -0.0586050116, 0.0292921141, 0.0235028658, 0.0198605619, -0.0492415391, -0.0333770253, 0.0233667269, -0.0431004204, 0.0303211175, 0.0426016785, -0.0623908341, -0.0463459902, 0.0333876386, -0.0218021367, 0.0477997363, 0.00130458875, -0.0390910096, 0.045586098, -0.000518045039, 0.0185856037, 0.00710966252, 0.0445063859, 0.0459480062, -0.0502649657, -0.0671171471, -0.0125900228, 0.0342965, -0.0342012234, 0.0208901055, 0.022779908, 0.0118563678, -0.0525732376, 0.0420009457, -0.0251431148, 0.0360923931, -0.0135671319, 0.0169787556, -0.0316518843, 0.0436588265, -0.00858402, 0.0292637032, -0.0401019976, -0.00701212604, 0.0784779638, -0.0292224232, 0.0502194092, 0.0387228429, -0.00729556568, 0.0285079703, -0.0220425278, -0.0289051123, 0.0169135556, -0.0375267118, -0.0507226251, -0.0425240584, 0.0217129979, -0.0399221368, -0.00328169158, 0.0423450544, 0.00211609853, 0.00494696293, 0.0431532189, -0.0112286443, 0.0595109835, -0.0111705456, -0.0319607705, 0.0202338863, -0.0214725044, -0.0187579654, -0.0445313863, 0.00824281573, -0.0266127456, 0.056597881, -0.0296416804, 0.0525112227, -0.0708077177, 0.0048918603, 0.082536079, 0.00258126482, 0.0561763905, 0.00137637684, 0.0331301093, 0.000448625738, -0.0316754244, -0.0181848016, -0.048071593, -0.0162563808, -0.0534097478, 0.0464613847, 0.0204281341, 0.0112474989, 0.0159462392, 0.0245726686, -0.00775676034, 0.0254615452, -0.0238330401, 0.0676176399, 0.0435102135, -0.0163914356, 0.00406374922, -0.0392783172, 0.0198938809, 0.0276571847, -0.0356005393, 0.0299738739, -0.0750760436, 0.0163141619, -0.0431799069, -0.0198457688, -0.0125609674, 0.000657009252, 0.0455705263, -0.0410741717, -0.0302927084, 0.0582017414, -0.0268416274, 0.0250764042, 0.0824490339, -0.01040556, 9.55914293e-05, -0.0168363843, 0.00210410054, 0.0184746683, 0.0270210449, 0.0202562176, -0.0299309194, -0.0257978104, -0.00579123, -0.013807944, -0.0267812535, -0.033447776, -0.0616762079, 0.0596937425, -0.0180670861, 0.063541688, -0.00261459826, -0.0480362959, -0.0327730142, 0.0105545921, -0.0212660749, 0.00573896943, 0.0176780932, 0.0120095555, -0.0163950827, 0.0217835158, 0.000350152899, -0.0551048927, 0.0146677298, -0.0437710881, -0.0215665251, -0.0204349328, 0.0550906733, -0.00172801688, 0.0596334487, -0.0737277493, 0.0244384166, -0.0595893301, -0.0518137626, -0.00245087501, -0.010358124, 0.00356610841, 0.0126558663, 0.016838491, -0.0389379859, 0.0665672, -0.0265984405, -0.0232025757, -0.0266347229, -0.0178335756, 0.0112768905, -0.0408898592, 0.0157178901, -0.0054387236, -0.0446892232, -0.0199839212, -0.0173096657, 0.004482002, 0.0733489767, 0.0123247243, 0.00937978551, 0.0219802763, -0.0184419677, -0.0552302226, -0.0479910746, -0.0343464278, 0.0456474423, -0.00321486848, -0.0174855627, 0.0400008596, -0.0128925471, -0.0108051319, -0.0663235411, -0.0492633469, -0.0364965908, -0.0332806632, -0.035478089, 0.0223687608, 0.0399038605, -0.0351804, 0.0057961, -0.0527927391, -0.153403744, 0.00620211, 0.0273938868, -0.0548539199, -0.0667159855, 0.0146010797, 0.0180603564, -0.00361924944, 0.0199710596, 2.32610655e-05, 0.0438686833, -0.0202661641, -0.00341573358, -0.035589356, 0.0423993729, -0.0551253036, -0.0290158316, 0.0453246608, 0.00210006279, -0.0410529077, 0.0627731904, 0.0459827371, 0.0612526238, 0.00806668866, -0.00808333047, 0.0475262552, -0.0436876304, 0.0096766511, -0.0291403588, -0.0307254922, -0.0312998518, 0.0488135405, -0.0270497967, -0.00784182269, 0.0296429936, 0.0610758625, 0.0408091024, 0.0418872684, -0.0606172346, 0.0182390474, -0.0266937185, 0.036363028, -0.0391470715, 0.0354181118, 0.038283065, 0.0420009419, 0.0162751582, -0.027732389, 0.00277053658, -0.0336731076, -0.0520906262, -0.0599587411, 0.0232307091, -0.0195024312, -0.0261510983, 0.018570004, -0.0429535657, 0.018082371, -0.0633163, 0.0320493355, 0.0235722791, 0.0285523552, 0.0304560326, 0.0305676796, -0.00773996348, -0.00563844945, -0.0174977221, 0.0401218534, 0.0190247931, 0.0192927141, 0.000702050573, 0.0636723787, 0.0583623499, -0.0364112109, 0.0108570829, -0.0281504206, 0.0343490206, -0.0104528982, -0.0471232571, 0.0542883575, -0.011982346, -0.0604843087, -0.0368120521, 0.0677057132, -0.0103591746, -0.0498724915, 0.00459269295, -0.00135733548, -0.0592990741, -0.0520972088, 0.000227761746, 0.0118289674, 0.0598156266, 0.0178998038, -0.010512094, -0.0134805581, 0.002773, 0.00121692475, -0.0231495816, -0.0361300819, 0.0358011238, -0.029766405, 0.031788785, -0.0246550851, -0.0518580712, 0.0333123468, 0.0340052322, 0.0349780917, -0.0634289831, -0.0355259888, 0.0239664726, 0.0178292375, -0.0016483072, -0.032749515, -0.00826817472, -0.0211097114, 0.013604396, 0.0105992714, -0.0173605271, 0.0349832699, 0.0189854372, -0.0306308605, -0.0684566349, 0.00344842416, 0.00916442461, -0.0239736028, 0.0377019085, -0.0149099473, -0.029182544, -0.0317424238, 0.024851311, -0.025551822, -0.0379614793, 0.0348514877, -0.0304480121, -0.0170495789, 0.061343275, -0.0270722862, 0.00400752528, -0.0247852448, 0.00993548054, -0.0401025973, -0.0732919723, 0.0193029512, 0.0352757, 0.0336065702, -0.0384167023, 0.0107739596, -0.0225006063, -0.0518251471, -0.00323525886, -0.0204584282, 0.014010381, -0.00108228181, -0.0557924025, 0.0122382175, -0.0577056296, 0.0441417694, 0.0104623726, -0.0126014054, 0.0494069681, 0.0414978042, -0.0234578438, -0.0176373478, 0.00218184828, -0.00946450885, 0.0357919857, 0.0569793023, 0.0174436644, -0.0441482738, -0.0625476763, -0.0406320021, 0.0519553497, 0.0312396865, 0.00757391099, -0.00368916825, 0.0728392377, -0.0639321804, -0.0141014, -0.0379721224, -0.0211040489, 0.0561437421, -0.0359966122, -0.0295489598, 0.0210246667, -0.00775768701, -0.0187684149, -0.0198752042, 0.00272974232, 0.00254566851, 0.0257657915, 0.0854167938, 0.0580376536, -0.0361023583, -0.0411539488, 0.0650602132, 0.00933412276, 0.0319420137, 0.021474449, 0.0343596153, 0.0470950082, 0.0618143268, 0.0239768829, -0.0123981722, 0.00794569682, 0.0205930267, -0.0254050549, -0.0128805405, 0.0219603628, -0.0676315725, -0.000261633628, 0.023031516, -0.0587540492, 0.0306633264, 0.0323457681, -0.0348652862, 0.0253181625, 0.00912696403, 0.0501239225, 0.015513299, 0.0239369571, 0.021509923, 0.0262993146, 0.0371772312, 0.0601696298, -0.00717260921, 0.0229723491, -0.0476154052, 0.0470779, 0.0114333564, -0.0170171577, -0.0259734485, 0.0444151722, 0.0193958338, 0.0326622389, 0.0021734226, -0.00556290615, -0.0115432646, -0.0486722, 0.0577039868, 0.0411552899, -0.0289813988, 0.0127422158, 0.0527458638, -0.0346235968, -0.0315800197, 0.0197456516, 0.0106016183, -0.0311284661, 0.0095728, 0.0650061443, 0.0258822832, 0.0421774462, -0.042142909, 0.054245051, -0.00795527734, 0.054050494, 0.0435884446, 0.0460058115, 0.0430817232, 0.062287353, -0.0296620615, -0.0180139113, 0.0160673279, -0.0550418496, -0.00656681648, 0.0734508112, 0.00297642569, 0.0486497916, -0.00680490211, 0.0484799184, 0.0348004512, 0.0550941639, 0.0419861041, -0.0340599418, 0.0664107427, 0.0238017049, 0.0394502915, 0.00515332958, -0.00292468653, -0.0146782938, -0.00664979313, 0.0503887981, -0.00114946661, 0.0399034955, -0.0416400805, -0.0014690219, -0.0195985772, -0.0181416608, 0.0459130742, 0.0397614799, -0.0201009512, -0.0324458815, 0.0814959556, -0.075804837, 0.016019756, 0.0106089553, -0.0152186723, 0.040463008, -0.0651006624, 0.0312351603, 0.00252594473, 0.0268551819, 0.0762491301, -0.00762512907, -0.0110683525, 0.0463345349, 0.030118404, -0.0548117869, -0.0239307452, -0.0476178266, 0.00428302819, -0.00319898478, 0.0400051363, 0.00301941135, -0.0728064179, -0.0353998654, -0.0365087, -0.0175266881, -0.0157892052, 0.0140508572, -0.0273292828, -0.0159290787, 0.0602735691, 0.0359141491, 0.0241083186, 0.00276457984, 0.00925926771, 0.0167256612, -0.0297229309, -0.0292472113, -0.0286070295, 0.0194637645, -0.0597240292, 0.0460928641, -0.044023674, 0.00717991311, 0.0198193844, -0.000390257424, -0.0605295561, 0.0301459748, -0.0031796298, 0.00220900518, 0.034077283, 0.0405443534, 0.0737753958, -0.0577186979, -0.0550092719, -0.0305972472, -0.0131154377, 0.0328825936, -0.0288712233, 0.0148119302, 0.0303324256, 0.0012461053, -0.0706132054, 0.034310881, -0.0293866154, 0.0318408236, -0.0752897859, 0.0662879422, 0.0385170467, -0.0347548202, -0.0724495947, 0.00171667919, 0.0196791254, -0.0624391548, 0.0172392782, 0.0141868684, 0.0418240093, 0.00872911606, 0.0211623609, 0.0081510935, -0.0396758392, -0.0257419012, -0.0426661633, -0.00111702888, 0.0397970416, -0.0706537813, 0.0356119, -0.0401154347, 0.0289262123, -0.000994167523, -0.0649490952, -0.0223681163, 0.00971384, 0.0194351282], metadata={'source': 'page_0_hnologies_ai-ml-services__tab6.txt', 'text': 'Home\\nAbout\\nCases\\nServices', 'url': 'https://www.microwebtec.com/technologies/ai-ml-services/#tab6'}, sparse_values=None), 'chunk-1': Vector(id='chunk-1', values=[-0.0217144676, -0.0296117421, -0.0185068212, 0.000188105405, 0.10476274, -0.049131643, 0.0115606161, 0.0566230156, -0.0192168243, -0.0272231493, -0.0121650873, 0.0482155643, -0.0286580101, 0.00168410328, -0.00253053987, 0.0350144655, 0.0259965919, -0.0297628753, 0.0584442131, -0.00134854496, -0.0440017618, -0.0389663391, 0.0067438446, 0.011415231, -0.0216914434, -0.00502302125, -0.0290898271, 0.0327642448, -0.0603078827, -0.0195580777, -0.00590922544, 0.0485134348, 0.00340518821, -0.0643970445, -0.0547732413, 0.0408149771, -0.0656543672, -0.0171962548, -0.0497308448, -0.0162332393, -0.0375824943, -0.0418495387, -0.0109997336, 0.0311860628, -0.0316861682, -0.00331069203, -0.0558540337, 0.0346804969, 0.00698148832, -0.0141385868, -0.0396610461, 0.030432187, 0.0585302673, -0.0273034386, -0.0366527587, 0.0216193665, -0.0449678302, -0.0482945666, -0.0329158418, -0.030353494, 0.0424822383, 0.0174404457, -0.0208503436, -0.0123955598, 0.0451305173, 0.0206601154, -0.0049602678, 0.018367162, -0.0717349276, -0.00714241574, -5.54688631e-05, -0.0267217904, -0.0492918491, -0.0211871415, -0.0396537408, -0.0165847875, -0.00726992777, 0.0698618442, 0.0448493287, 0.0139952647, -0.0212995782, -0.0110590644, 0.0282863695, 0.0290961899, 0.0427858457, -0.0174932629, -0.0317357257, 0.0103938682, -0.0431367867, 0.0292735938, -0.0111806719, -0.0532309227, 0.0414467789, 0.0483421683, 0.0482158735, -0.029534867, 0.0191284176, -0.044013612, 0.0119182076, -0.00544527173, -0.0647670254, -0.0322224908, -0.0136136841, -0.00113382912, -0.0636294335, -0.0379674658, -0.0194067918, 0.010317497, -0.044028651, -0.00449772412, -0.0607415698, 0.0115589658, -0.0178411473, -0.00512851356, -0.0181994848, 0.0422833078, 0.0130438637, -0.00431377813, -0.022489246, -0.00186937884, 0.0201790556, 0.051347971, 0.0250611566, 0.0635745898, 0.0438755043, 0.0489493497, 0.0532859117, -0.00298915035, 0.00609192299, -0.0302894693, 0.0183646567, 0.0326227434, 0.023798164, -0.0485233, -0.0180591, -0.0239230301, -0.0221462604, -0.0377778076, 0.0398617573, -0.0102428822, -0.0406836756, 0.0543588474, 0.0022114031, -0.0520315133, 0.0418804251, -0.0283871256, 0.00693627, -0.0213174596, -0.041636318, 0.0708462745, -0.0152635537, 0.0348140895, 0.0340556651, -0.0387780704, -0.010688765, 0.0737869665, 0.0886972696, -0.0202773586, -0.041892875, -0.0183855873, 0.00623016851, 0.0302018747, 0.0370715335, 0.0159579851, -0.0407902896, 0.0142320218, 0.028319627, -0.0569920093, 0.0412549637, 0.0175705329, -0.048434291, -0.0415291823, 0.0151133575, -0.0155027462, 0.00837270264, 0.0490094908, 0.0391484462, 0.012893266, -0.0109785628, 0.00361605478, -0.0485257506, 0.0417351797, 0.0431288965, 0.039687857, -0.0692675859, -0.023010321, 0.0555770174, -0.0525698476, -0.0216337703, 0.0395019911, -0.0782608762, -0.0444327779, 0.023707984, -0.0211295988, 0.042345345, -0.0101630352, -0.0402904749, 0.0693166554, 0.0168717206, 0.000531535596, 0.0678377375, 0.0140267713, -0.00570686162, -0.0475982837, -0.0693168417, -0.0305550937, 0.0220734626, -0.0468781181, 0.0222755279, 0.0117412126, -0.0204552319, -0.0467165187, 0.0368543, -0.0256004799, 0.0563758686, -0.0369792879, 0.0127138691, -0.0317546092, 0.0494958647, -0.00124883582, 0.0491499603, -0.0387628078, 0.0142269433, 0.0379849263, -0.0126615195, 0.0168716796, 0.0538459308, -0.0281639379, 0.0255733337, 0.00801033434, -0.036622759, 0.0114856986, -0.0421936437, -0.0590386912, -0.0322802104, 0.0153565072, -0.008462606, -0.0259414949, 0.0552029125, -0.00923609734, -0.0155751044, 0.0285949502, -0.0478129275, 0.0566687249, -0.0207560826, -0.018749522, 0.0209635664, -0.0169828963, -0.0243793223, -0.00855711568, -0.020558482, 0.00204820256, 0.0245277379, -0.027604755, 0.0256934743, -0.0431694649, 0.0398487449, 0.079378821, 0.00448969612, 0.0422194488, 0.00504670618, 0.013128669, -0.0378994979, -0.0278724357, -0.0202784371, -0.0480589308, -0.0106209684, -0.0469550192, 0.0370538533, 0.0166347083, 0.0285744369, 0.0377624221, 0.0381624363, 0.0237332676, -0.0135140345, -0.0554436557, 0.0628249943, 0.00490265712, -0.0265731607, -0.00332044065, -0.0161299761, 0.0577472411, -0.0176771246, -0.0231117755, 0.0180036444, -0.0516433269, 0.045367483, -0.0395299867, -0.0105957836, -0.0555153824, 0.00433555059, 0.0423693359, -0.0465201735, 0.000276564591, 0.042479597, -0.029737791, -0.00483499188, 0.0458325632, -0.0117405178, 0.0329466052, -0.0124097737, 0.0146719674, 0.00595995598, 0.0141349807, 0.00975916814, -0.0545039624, -0.00148476847, -0.0157164913, -0.0210661292, 0.0395607837, -0.0149688683, -0.0524586365, 0.0043282914, -0.0101016853, 0.0525006764, -0.00595117174, -0.0438867472, -0.0429622643, -0.0365261212, -0.0227867607, -0.0107252365, 0.00761030428, 0.0260994509, -0.0148683619, 0.00979885831, 0.00682334322, -0.0320766754, 0.0229194965, -0.0388059206, -0.0170951393, -0.00183618616, 0.0567781813, 0.0174821839, 0.0411987528, -0.0462396964, -0.00436081225, -0.0280148108, -0.0428435616, -0.0291506834, -0.0365430117, 0.0101503432, 0.0315366611, 0.0157445092, -0.0527821, 0.0510390066, -0.00930318888, 0.0138080129, -0.0559593923, -0.0233355798, -0.023977451, 0.0163347349, 0.0146795632, 0.0162226856, -0.0372151658, -0.0293059964, -0.0150753548, -0.00610440876, 0.0336859412, 0.0385297723, 0.00486592948, 0.000576260616, -0.00148617721, -0.0473963656, -0.0387316644, 0.0264608394, 0.00132106873, -0.0284868609, -0.0173953939, 0.0389153771, 0.00259515876, -0.0299609508, -0.0367812291, -0.0167362802, -0.0464631878, -0.0425542891, -0.0260409, 0.0318899751, 0.0410422869, -0.0177493375, 0.0275497064, -0.0447676368, -0.157406837, -0.0333468579, -0.00399043, -0.029574139, -0.0257084556, 0.00490661431, 0.054202009, -0.0592176728, -0.00639718538, -0.000399906363, 0.02359825, -0.0348061286, -0.0216351897, -0.0427384302, 0.0217922721, -0.0364746861, -0.0330975, 0.0665061846, -0.0622533411, 0.0233645216, 0.0419938192, 0.0673065931, 0.0608880967, 0.0100501245, 0.00786714349, 0.00624772254, -0.0275576767, -0.0180109441, -0.0305740554, 0.000347452908, -0.0374622382, 0.0311143585, 0.018145863, -0.0155577064, -0.00454290956, 0.0444369726, 0.0762921199, 0.0477568954, -0.0195371639, 0.0416425429, -0.0319907293, 0.00655990327, -0.00293985242, 0.00129444851, 0.0377332307, 0.0294632111, 0.0200656131, -0.0754386, 0.027581498, -0.00071819243, -0.0663952455, -0.0447671115, 0.0130373612, -0.0154611673, -0.0414047651, 0.0333336443, -0.0499073192, -0.0146578159, -0.02111, 0.0215855632, 0.0150279505, -0.00479921745, 0.0401531979, -0.0025534078, -0.0526409261, -0.013373062, -0.0467377864, 0.032636, 0.0259575639, 0.0406929515, 0.00921887, 0.0506799854, 0.0435356498, -0.0287083946, 0.0491608679, 0.00399061712, 0.0403857753, -0.00346065173, -0.053242363, -0.00194811204, -0.0191290751, -0.0187276546, -0.0415821709, 0.0695987791, 0.0100872386, 0.000587761053, -0.0486503765, 0.0181216579, -0.0933083594, -0.010684764, 0.0147939185, 0.00623519626, 0.0381045, 0.0114783365, -0.0573774688, -0.0182311572, 0.00112384569, 0.00292898831, -0.0422762744, -0.0248438362, 0.0312168486, -0.0054061478, 0.0237974916, -0.0121564614, 0.0111548631, -0.00864878111, 0.0398659483, 0.0268049594, -0.0163770076, -0.00958970562, 0.0567941479, 0.0752931, -0.0366448089, -0.102175288, -0.0182629153, -0.0306284819, 0.048861187, 0.00320638972, 0.0117180478, 0.0291328412, 0.00689609302, -0.014864319, -0.0438976623, 0.0417365618, 0.0827455893, -0.0270694252, -0.0200154111, -0.0490538478, -0.00792746525, -0.0451968722, 0.021340197, 0.00565332966, -0.0228835605, 0.0379806086, -0.0573202036, -0.00797429401, 0.00811104104, -0.00591615867, -0.0489482097, -0.0377197154, 0.0142203132, 0.00439111702, -0.0451822, 0.0376203023, 0.0526954941, 0.012584988, -0.0409253575, -0.00308529637, -0.0219063982, -0.0388057679, -0.00766124157, -0.0319773108, 0.0309828352, 0.00762628438, -0.0127866464, -0.00426011579, -0.0471075438, 0.0706684217, -0.00881773513, -0.0211189035, 0.051231239, 0.046065364, 0.0132169724, 0.011054079, 0.0141427899, 0.0016720671, 0.0347984843, 0.00130380946, 0.0220550075, -0.0545625947, -0.0419966467, -0.0152795566, 0.0228376761, 0.0122027686, -0.000425076869, -0.0259672888, 0.050234247, -0.0404771119, -0.0233373679, -0.0302391946, -0.0178795941, 0.0552047752, -0.0527155213, -0.0152460365, -0.002902915, -0.0320288911, -0.0467778891, -0.0221393313, 0.0311265402, -0.0131080467, 0.0296804924, 0.0594575107, 0.0568624362, -0.0509222634, -0.0693685934, 0.0584367178, -0.00273360708, 0.0135424882, 0.0231434498, 0.0450287759, 0.0527530462, 0.072535567, 0.001505155, -0.0159164276, -0.0376024097, 0.0328248292, -0.0299575627, -0.0264385566, 0.0400056206, -0.0246598534, 0.0111732446, 0.0225590412, -0.0310693793, 0.062524341, 0.0461523347, -0.0469338633, 0.0546640046, 0.0112032248, 0.0351073816, 0.0285354741, 0.00618690439, -0.000804981159, 0.00268495851, 0.0239780769, 0.0316289514, -0.0243973527, 0.0112629682, -0.0417618267, 0.067042686, 0.0487866625, -0.023242902, -0.0334474146, 0.0344997793, 0.000391625363, -0.0160600301, 0.0541634969, -0.0628615245, -0.0121026402, 0.00210030447, 0.00268436433, 0.064803265, -0.0657965541, -0.00439338386, 0.0408182815, -0.0526560619, -0.0198868345, 0.0344378203, -0.0157176014, 0.00289642345, 0.0226833932, 0.0528399609, 0.0349024646, 0.0357851088, -0.0136639234, 0.0330660306, -0.031841211, 0.0520342328, 0.0137387114, 0.0279745013, 0.0487384684, 0.0792453811, -0.0479828082, -0.0316095725, 0.0416555069, -0.0302191302, 0.0190899279, 0.0468491949, -0.0102320565, 0.0626775697, 0.0102242846, 0.095349662, 0.0249467548, 0.0592997521, 0.0333331861, -0.00903358497, 0.0120611088, 0.0421517156, 0.0740793124, -0.000711786211, -0.020305017, -0.0124305356, -0.0151320482, -0.00263914699, -0.0181597844, 0.0247749016, -0.0157239456, 0.0317258425, -0.0382895954, -0.0206737407, 0.0734567419, 0.0249165017, -0.0285012089, -0.0288323686, 0.0719412416, -0.0842158645, 0.0235850047, 0.0046903803, 0.0270338, 0.0419623107, -0.0369701423, 0.0665333569, -0.00500197615, 0.0585068539, 0.063195549, 0.00205121352, -0.0145341689, 0.0166341867, 0.0316589624, -0.0557994507, -0.0365207046, -0.0248550959, -0.0154146217, -0.00876256078, 0.0194918085, -0.00980310421, -0.00897998, -0.0329188257, -0.0232769, -0.0138644595, 0.0455125682, 0.00232834462, -0.0183832906, -0.0212514475, 0.0238290429, -0.00371526065, 0.013465764, 0.0586759932, 0.00291817612, 0.0194976665, -0.0208702348, -0.0111150807, 0.00753126387, 0.00553700048, -0.0582072176, 0.0479818135, -0.0391390547, 0.0214417912, 0.0403296314, -0.0267512407, -0.0370843634, 0.0401238948, -0.0228547417, 0.0360470153, 0.0536494367, 0.0328135, 0.0366206244, -0.023097463, -0.0208618715, -0.0322502926, 0.0193011686, 0.0372804664, 0.016396448, 0.0358712226, 0.0244313646, 0.0177557524, -0.0571598858, 0.0362106413, -0.0361166783, 0.0444799922, -0.0450412, 0.086754784, 0.00947802328, -0.0189750697, -0.0452540107, 0.0594920963, -0.00656184461, -0.054683648, 0.0342196748, 0.0368195102, 0.0502772853, 0.0166969225, -0.00616870448, 0.00610418059, -0.0375060849, -0.0204344187, -0.0445151143, 0.0500901118, 0.0207740813, -0.0862009, 0.0423793606, -0.0460868143, 0.0355347879, 0.0190205146, -0.0430541299, -0.0608488545, 0.0161956493, 0.0444815755], metadata={'source': 'page_0_hnologies_ai-ml-services__tab6.txt', 'text': 'TechnologyTechnology SubCloud Application Development ServicesAzure DevOps ServicesAI & ML ServicesShopify DevelopmentGolang Development ServicesDevOps Consulting ServicesWebflow DevelopmentBusiness TransformationLaravel Application DevelopmentSymfony Web DevelopmentNode.js DevelopmentAngularJs Web Development ServicesRuby on Rails Application DevelopmentMicrosoft DevelopmentMobile Application DevelopmentIoT and Embedded Systems & Smart SolutionsCloud TechnologyReactJS DevelopmentDrupal Web', 'url': 'https://www.microwebtec.com/technologies/ai-ml-services/#tab6'}, sparse_values=None), 'chunk-2': Vector(id='chunk-2', values=[-0.0161489584, -0.0288910456, -0.0388099514, -0.0202457644, 0.084393926, -0.0389299579, 0.00558274332, 0.0331466757, -0.0146191856, -0.00138541171, -0.0299783796, 0.0628336146, -0.0285420846, -0.00416631624, -0.0176435821, 0.056395188, 0.035613928, -0.006580899, 0.0336531363, -0.0214300342, -0.0513221174, -0.0515074842, 0.0334053747, 0.000989133609, -0.00829971209, -0.00264272327, -0.0108528025, 0.0414537787, -0.0586266667, -0.0669220239, 0.00146145991, 0.0307973716, -0.00610455498, -0.0508448295, -0.0222202223, 0.046464432, -0.0990820378, -0.00444220286, -0.0235732943, -0.0211161859, -0.0369261764, -0.0546551906, -0.00686026132, 0.0219678637, -0.024861943, -0.0153560815, -0.0455639921, 0.0311332643, 0.00165387313, -0.00198781281, -0.0471257977, 0.0402475893, 0.0529304892, -0.0183366183, -0.0342828818, 0.0292064622, -0.0251625758, -0.0262825, -0.0449397787, -0.0172115322, 0.0801719055, 0.0424852856, -0.0409670398, -0.0133675588, 0.0257634278, 0.0136324475, -0.00786783732, 0.0191301033, -0.0448289588, -0.0163742937, -0.0121042952, -0.0292048734, -0.0360980965, -0.0272259507, -0.0259292033, -0.0181483142, 0.00470526889, 0.0358863473, 0.0326405168, 0.00911050569, -0.00432250043, 0.00539337331, 0.0421174951, 0.0223232396, 0.0262981374, -0.0228912216, -0.0382471643, 0.0171637684, -0.016244432, 0.0366378501, -0.0234457683, -0.0402636752, 0.0466109924, 0.0540442653, 0.0607899949, -0.0104574729, 0.0240361504, -0.0425353348, 0.0180654284, -0.0107607869, -0.0407829881, -0.0337983668, -0.0133149112, -0.00988637563, -0.0176613312, -0.0564984232, -0.0148581509, 0.0227142088, -0.0576230884, 0.0158526804, -0.0723961592, -0.00581268687, -0.0228534546, 0.00299095479, -0.0510933101, 0.0743554533, 0.00276403269, 0.0105767995, -0.0181932412, 0.0107534705, 0.0161095634, 0.0689259619, 0.0190754272, 0.0624438487, 0.0564818457, 0.0668851286, 0.0465114452, 0.0102014812, -0.00808172766, -0.0292507187, 0.0602506, 0.0130980993, 0.0247014351, -0.0354456231, -0.0134692844, -0.0219751373, -0.0148815149, -0.0335406736, 0.0312644392, 0.0020178773, -0.0195814781, 0.0502853692, -0.00088838645, -0.0580932572, 0.0250448249, -0.0325981677, 0.00903950445, -0.0336138792, -0.0499964058, 0.0518279187, 0.00405331282, 0.0231548119, 0.040551465, -0.0285181012, -0.028698571, 0.0656986386, 0.075955689, -0.0215594936, -0.0320413262, -0.0133243958, 0.0238246974, 0.0289043412, 0.0298607778, -0.00233346759, -0.0392187834, 0.00541259255, 0.0265501104, -0.0489933454, 0.0404983871, 0.0326332077, -0.0515309572, -0.0170284491, -0.00121250178, -0.00934079103, -0.01645579, 0.056299042, 0.00756376842, 0.0278535746, -0.0158087835, 0.0036588239, -0.0638749897, 0.0356308967, 0.0479560941, 0.0437267795, -0.0464199, -0.0476644672, 0.0458390824, -0.0612320639, -0.00266241771, 0.0390904546, -0.0687205866, -0.0156593956, 0.000646187, -0.0106087457, 0.0272539314, -0.027140541, -0.0482766926, 0.0651834384, 0.0596764833, 0.014986136, 0.0445031561, -0.00163450243, 0.0392927937, -0.0334077962, -0.0413975567, -0.0452477485, 0.0428251065, -0.0473165549, 0.0217692666, 0.0239787381, 0.00393975619, -0.052023191, 0.0399186052, -0.00756870862, 0.0665914863, -0.0297776703, 0.0385364927, -0.0390588492, 0.0334421806, 0.0105214147, 0.0527700186, -0.0359157771, 0.00454467768, 0.0273380484, -0.0167459138, 0.0617469326, 0.0311198141, -0.0358255953, 0.0386956036, 0.0201505404, -0.0769469291, 0.0276040416, -0.00882830471, -0.0522976145, -0.0560108609, 0.0265978631, -0.024327714, -0.0297415704, 0.0688874796, -0.0113437176, 0.0099298656, 0.0162360761, -0.0129894419, 0.0296754781, -0.00936550554, -0.0092645837, 0.0361291729, -0.0214226339, -0.0483845547, -0.00973817147, -0.0205877703, 0.0135248154, 0.00243600016, -0.019754773, 0.0405457504, -0.0398158506, 0.0450396538, 0.0678326339, 0.00335277431, 0.0410634689, 0.0115523292, 0.0231575109, 0.000794374442, -0.0130567113, -0.0296479, -0.0525040068, -0.0292802826, -0.0472172797, 0.0384202078, 0.0503945947, 0.0289441086, 0.0248801336, 0.0108557725, 0.0191538427, -0.00463588862, -0.0417052917, 0.082507506, -0.00706348708, -0.0338634402, -0.0221851412, -0.0162770357, 0.0465606786, -0.0446059555, -0.0386690348, 0.0372882113, -0.0361286737, 0.0336799286, -0.0404276177, -0.0141213238, -0.0331153087, 0.00735538919, 0.0397089347, -0.0484905578, -0.00923502, 0.0501296371, -0.0400680453, 0.015813766, 0.0237378515, 0.00124767411, 0.0504510291, -0.00741684297, 0.0134509616, -0.00809263, 0.0225321744, -0.00698951026, -0.0406343639, 0.00025040211, -0.0445626602, -0.0220105927, 0.00300888717, -0.000168390849, -0.0432956293, 0.00910407584, -0.010602951, 0.0516335219, 0.00722877868, -0.0298246592, -0.0330709182, -0.014523346, -0.0325231627, -0.0194931589, 0.0310150012, 0.0456978716, 0.00587243307, 0.0195963718, 0.0152530046, -0.0345755182, 0.0409522019, -0.0498599447, -0.0200631469, -0.00808596425, 0.0260546338, 0.0180116277, 0.014895767, -0.0466359966, -0.0204418879, -0.0304559097, -0.0379916579, -0.0222960897, -0.0203221496, 0.0109345559, 0.0465080254, 0.00755341165, -0.0577920713, 0.091536954, -0.0388749763, -0.02575255, -0.0459179915, -0.0201753732, 0.00248811953, 0.0127339549, 0.0187510494, 0.0278983284, -0.0401675887, -0.00967683643, -0.00258371513, -0.0168543272, 0.051232513, 0.0627109334, 0.00737834116, 0.0165856723, -0.00384117872, -0.020845864, -0.0378017649, 0.0219664164, 0.0138987824, -0.00743522542, -0.0336920395, 0.0537086353, -0.0237845648, -0.0439439379, -0.0405863933, -0.0442656651, -0.0476241708, -0.0197966825, -0.0421974771, 0.0186681226, 0.0408432297, -0.00794600882, 0.0315236226, -0.026398221, -0.163780913, -0.0178527702, 0.0142524857, -0.0263604261, -0.0433515087, 0.0199453551, 0.0144777773, -0.0387847386, 7.77240493e-05, 0.0098473765, 0.0556937568, -0.0270831287, -0.0336607881, -0.0293632727, 0.00717198849, -0.0191096421, -0.0526726283, 0.0662971884, -0.0738557205, 0.0037200693, 0.0446831733, 0.0542021617, 0.0516710952, 0.0251269229, 0.0149407955, 0.00803237874, -0.0448303223, 0.00162659655, -0.0391135849, -0.0147492094, -0.0233572274, 0.0534579754, 0.00778166438, -0.0219411142, -0.0118009737, 0.0384567, 0.0575647131, 0.0354775675, -0.010696508, 0.0312512368, -0.0196478534, 0.0173697919, -0.0123748444, 0.0240107588, 0.0541783497, 0.00483088614, 0.0115611777, -0.0540082045, 0.0298363063, -0.00866857823, -0.0327996127, -0.0465222299, 0.0163258482, -0.0417354479, -0.0425947495, 0.046469111, -0.0335215926, -0.0266952422, -0.0283255, 0.0375203267, 0.0294270068, 0.0183771588, 0.0333303139, -0.00842080545, -0.0369363427, -0.017975295, -0.0547031723, 0.038364388, 0.0369357578, 0.0202448666, 0.00606771745, 0.0452770218, 0.0336238109, -0.00209790119, 0.0441012643, -0.0197489802, 0.020482434, -0.00796160381, -0.0367689729, 0.000968820939, -0.016826449, -0.0369849056, -0.0411259457, 0.0660748854, -0.0173774399, -0.00150126719, -0.0587495975, 0.0049690688, -0.0664705485, -0.0305707119, 0.00960129499, 0.00668274565, 0.0321975313, 0.0395238884, -0.0404136516, -0.00970273465, -0.00826023333, 0.0129651856, -0.014733674, -0.0666109771, 0.0214230269, -0.0161434654, 0.00725747133, -0.0204342958, -0.01265621, 0.0208104383, 0.0140080396, 0.0388666019, -0.0458680131, 0.00171417522, 0.0393735878, 0.0360315256, -0.0470523648, -0.0411626957, -0.0222493857, -0.0477956943, 0.0387910753, 0.0164726768, 0.0275499132, 0.0316433422, 0.00490626926, -0.0420773141, -0.0663542449, 0.0415470339, 0.0543727838, -0.0482825823, -0.0366906226, -0.064939484, -0.0172329, -0.0677641556, 0.022270726, -0.00840384699, -0.0149464104, 0.0345471092, -0.042354729, -0.00224558474, 7.48246384e-05, -0.0161458626, -0.0368574634, -0.0634197146, -0.0150510874, 0.00561669469, -0.03623439, 0.0402263552, 0.0190360304, 0.0469060577, -0.0442349352, -0.0127287107, -0.0347166881, -0.053061191, 0.018430857, -0.0380601622, 0.0229756031, -0.0157095883, -0.0410101786, -0.00111575052, -0.0535741262, 0.0492994338, 0.00225368096, -0.0210308526, 0.0555626303, 0.0325184241, 0.00857433, -0.015953077, 0.0113646965, -0.00546492822, 0.0517011471, 0.019977374, 0.0104000373, -0.0488287583, -0.0208821222, -0.0194988623, 0.0168655124, -0.00217699423, -0.0328299217, -0.0186077449, 0.0692056119, -0.0161917731, -0.042032931, -0.0414776616, -0.0372112133, 0.0662484318, -0.0529273637, 0.00208680844, -0.0144962529, -0.0461122319, -0.0318719633, -0.0291783903, 0.030577803, -0.0233330093, 0.0279039852, 0.0566683412, 0.0443024524, -0.00620113267, -0.0551685952, 0.0451684967, 0.00944034383, 0.0251527615, -0.00075215142, 0.0228230432, 0.0464134589, 0.0687450618, 0.0212226771, -0.0183682442, -0.0329668671, 0.0436825752, -0.0486097857, -0.0493029878, 0.0474654473, -0.040230114, 0.0209537987, 0.0197577886, -0.0261696838, 0.0423564762, 0.0359620042, -0.0689047724, 0.0375816822, 0.0156954899, 0.0413277298, 0.0428075045, 0.00917958748, 0.00469016423, -0.00184979872, 0.0266485158, 0.0370376818, -0.0266962778, -0.00697339606, -0.0153317358, 0.0553433374, 0.0591962, -0.0404080786, -0.0156929903, 0.0686177, -0.00881496258, -0.00584875047, 0.061627198, -0.0435948968, -0.00598863605, -0.0052918708, 0.0351144373, 0.0314325318, -0.0526402593, -0.0153902341, 0.0648675635, -0.0536105298, -0.0132143367, 0.0410513468, -0.0216218531, 0.00326464348, 0.0381288566, 0.0527923331, 0.0282353554, 0.0407257788, -0.0306648184, 0.0318878815, -0.0549313501, 0.0392956138, 0.0208238065, 0.0179540608, 0.0414155722, 0.0540135093, -0.0279670805, -0.00964180194, 0.050353989, -0.0309114326, 0.0123135718, 0.0421368033, 0.00849240832, 0.0466946661, 0.00210880628, 0.062036708, 0.0200031772, 0.0344079919, 0.0204073377, 0.000647204812, 0.0321587212, 0.0496341661, 0.0757233426, -0.013094685, -0.00642213318, 0.00688410737, -0.0240559336, 0.00327863917, -0.00601079967, 0.0358370729, -0.0142330816, 0.0238778517, -0.0396032892, -0.0257142484, 0.0677443519, 0.0343948156, -0.0431343317, -0.0465701222, 0.0666410699, -0.0521604046, 0.0181481056, 0.0100784106, -0.0125860423, 0.0580635406, -0.0442063883, 0.050640095, -0.0128659317, 0.0534864776, 0.0548165813, -0.00191055122, -0.0269794, 0.0413036607, 0.0148592554, -0.0684019253, -0.0122672031, -0.0225197989, -0.0200800598, -0.0044996473, 0.0259388834, 0.0201003104, -0.0427507572, -0.037547376, -0.0442307666, 0.0202268548, 0.0378302522, -0.00858404953, -0.0304197744, -0.0307057295, 0.0354367271, -0.0039127958, 0.00126397202, 0.0377992839, 0.0281266831, 0.0265792608, -0.0211783797, -0.0200410075, 0.0193724651, 0.00393483182, -0.0651967898, 0.0505087972, -0.0584316, 0.00389372814, 0.0309174303, -0.0240271334, -0.051993385, 0.0254495088, -0.0433248766, 0.00303851557, 0.0345629863, 0.0542946979, 0.0441743694, -0.0622301474, -0.0219493806, -0.0364082344, 0.00145583553, 0.0287095904, -0.00965838227, 0.0389299393, 0.0299054813, 0.0361515, -0.0512611084, 0.0577036403, -0.0479446948, 0.0588625297, -0.0177098121, 0.0731148049, 0.034291096, -0.0108405603, -0.0448148884, 0.0524893, 0.0113542248, -0.0643780828, 0.0454006046, 0.0606217049, 0.0586045198, 0.011547666, -0.0277219377, -0.00373706804, -0.0304173678, -0.0104456162, -0.0164924264, 0.0678363517, 0.0102542825, -0.0949218, 0.0331070311, -0.0132080317, 0.0224474929, 0.0431412868, -0.0513253026, -0.0575246066, -0.000885673158, 0.0462275408], metadata={'source': 'page_0_hnologies_ai-ml-services__tab6.txt', 'text': 'TechnologyReactJS DevelopmentDrupal Web Development Services2D and 3D Video AnimationUI & UX DesignWeb DevelopmentEnterprise SolutionsDigital MarketingSoftware Outsource to IndiaGraphic DesigningeCommerce DevelopmentWordPress DevelopmentWooCommerce DevelopmentShopify DevelopmentPython Development', 'url': 'https://www.microwebtec.com/technologies/ai-ml-services/#tab6'}, sparse_values=None)}, usage={})\n"
     ]
    }
   ],
   "source": [
    "# Example: fetch first 3 vectors\n",
    "ids_to_fetch = [f\"chunk-{i}\" for i in range(3)]\n",
    "\n",
    "fetched = index.fetch(ids=ids_to_fetch)\n",
    "print(fetched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a422c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"No API_Key found, Please set the API_KEY.\")\n",
    "    exit(1)\n",
    "elif API_KEY.strip() != API_KEY:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4cb9a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = OpenAI(api_key=API_KEY, base_url =BASE_URL)\n",
    "MODEL = \"llama3-70b-8192\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc0a202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Bot: EverythingBegins With A Hello!\n",
      "ü§ñ Bot: Based on the given context, the service offered by Microweb is:\n",
      "\n",
      "* Hire a Bookkeeper Services\n",
      "ü§ñ Bot: Based on the context, Microweb provides \"Hire a Bookkeeper Services\".\n",
      "ü§ñ Bot: Based on the context, the services offered by this company are:\n",
      "\n",
      "1. Organizational design\n",
      "2. Change management\n",
      "3. Culture transformation\n",
      "4. Leadership development\n",
      "5. Strategic planning\n",
      "ü§ñ Bot: I'm happy to help! However, I notice that there is no specific question asked. The provided context appears to be a repetitive sequence of links and a input field with a message to leave it empty. Could you please clarify or provide an actual question related to this context? I'll do my best to assist you.\n",
      "ü§ñ Bot: I'm happy to help! However, I notice that there is no specific question provided. The context appears to be a repeated pattern of website footer links and a input field with a placeholder text. Could you please clarify or provide the actual question you'd like me to answer? I'm here to assist you!\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    query_vector = model.encode(f\"query: {query}\", normalize_embeddings=True).tolist()\n",
    "\n",
    "    results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=5,  # Get top 5 most relevant chunks\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "    for match in results[\"matches\"]:\n",
    "        context += match[\"metadata\"][\"text\"].strip() + \"\\n---\\n\"\n",
    "\n",
    "    \n",
    "    # üîÅ Add memory to prompt\n",
    "    history_string = \"\"\n",
    "    for turn in chat_history[-3:]:  # Use last 3 Q&As (for brevity)\n",
    "        history_string += f\"User: {turn['question']}\\nAssistant: {turn['answer']}\\n\"\n",
    "\n",
    "    # Final prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    Use the following context to answer the question.\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        streamed_response = groq_client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers clearly and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            # stream=True   \n",
    "        )\n",
    "        answer = streamed_response.choices[0].message.content\n",
    "        # return answer\n",
    "\n",
    "        # Save to memory\n",
    "        chat_history.append({\"question\": query, \"answer\": answer})\n",
    "        \n",
    "        print(\"\\nü§ñ Bot:\", answer)\n",
    "\n",
    "        # result = \"\"\n",
    "        # for chunk in streamed_response:\n",
    "        #     content_piece = chunk.choices[0].delta.content or \"\"\n",
    "        #     result += content_piece\n",
    "        #     cleaned_result = result.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        #     yield cleaned_result  # <- Streaming to Gradio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM Error] {e}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
