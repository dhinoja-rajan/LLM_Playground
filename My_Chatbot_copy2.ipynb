{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d1c168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai pinecone-client tqdm sentence-transformers pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69388c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìÅ Standard libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import socket\n",
    "import ipaddress\n",
    "\n",
    "# üß™ Third-party libraries\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv \n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "# üåê Selenium (Web Automation)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# üß† AI/NLP\n",
    "from openai import OpenAI\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# üîç Vector DB\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd323211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory where your scraped text files are saved\n",
    "data_dir = \"scraped_pages\"\n",
    "\n",
    "# This will store all your documents\n",
    "documents = []\n",
    "\n",
    "# Loop through all text files in that folder\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read().strip()\n",
    "            \n",
    "            # Split by lines\n",
    "            lines = raw_text.split(\"\\n\")\n",
    "            \n",
    "            # Extract URL from the first line\n",
    "            url = None\n",
    "            if lines and lines[0].startswith(\"URL:\"):\n",
    "                url = lines[0].replace(\"URL:\", \"\").strip()\n",
    "                content = \"\\n\".join(lines[1:]).strip()\n",
    "            else:\n",
    "                content = raw_text  # fallback if URL line not found\n",
    "            \n",
    "            # Save as structured dict\n",
    "            documents.append({\n",
    "                \"filename\": filename,\n",
    "                \"url\": url,\n",
    "                \"content\": content\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9078187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be750c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    split_chunks = splitter.split_text(doc[\"content\"])\n",
    "    for chunk in split_chunks:\n",
    "        chunks.append({\n",
    "            \"chunk\": chunk,\n",
    "            \"source\": doc[\"filename\"],\n",
    "            \"url\": doc[\"url\"]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c3f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773f3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ‚úÖ Load the model\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# ‚úÖ Prepare text with \"passage: \" prefix (required by E5)\n",
    "texts_to_embed = [f\"passage: {chunk['chunk']}\" for chunk in chunks]\n",
    "\n",
    "# ‚úÖ Batch encode all 7,339 chunks at once\n",
    "vectors = embedding_model.encode(\n",
    "    texts_to_embed,\n",
    "    batch_size=64,                 # ‚ö° Increase if you have more RAM/GPU\n",
    "    show_progress_bar=True,\n",
    "    normalize_embeddings=True      # ‚úÖ Normalize for cosine similarity\n",
    ")\n",
    "\n",
    "# ‚úÖ Attach embeddings to your chunks\n",
    "for i, vec in enumerate(vectors):\n",
    "    chunks[i][\"embedding\"] = vec.tolist()  # Optional: convert to list if saving to JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8626df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv(override=True)\n",
    "# if not load_dotenv():\n",
    "#     print(\"Warning: .env file not loaded. Make sure it exists.\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "# Replace with your real values\n",
    "PINECONE_API_KEY = PINECONE_API_KEY\n",
    "\n",
    "if not PINECONE_API_KEY:\n",
    "    raise ValueError(\"PINECONE_API_KEY environment variable not set\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b8c1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "# Configuration\n",
    "index_name = \"chatbot-index\"\n",
    "dimension = 768  # for E5-base-v2\n",
    "metric = \"cosine\"\n",
    "\n",
    "# ‚úÖ Check if index exists, else create it\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=dimension,\n",
    "        metric=metric,\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",             # Based on Pinecone environment info\n",
    "            region=\"us-east-1\"       # your region from Pinecone Console\n",
    "        )\n",
    "    )\n",
    "\n",
    "# ‚úÖ Connect to index\n",
    "index = pc.Index(index_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9d69a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 100  # Upsert in batches (efficient & safe)\n",
    "batch = []\n",
    "\n",
    "for i, chunk in enumerate(tqdm(chunks)):\n",
    "    vector_id = f\"chunk-{i}\"  # Unique ID for Pinecone\n",
    "\n",
    "    vector = {\n",
    "        \"id\": vector_id,\n",
    "        \"values\": chunk[\"embedding\"],  # The 768-dimensional vector from E5\n",
    "        \"metadata\": {\n",
    "            \"text\": chunk[\"chunk\"],     # Original text chunk\n",
    "            \"source\": chunk[\"source\"],  # File or page name\n",
    "            \"url\": chunk[\"url\"]         # Optional if you have it\n",
    "        }\n",
    "    }\n",
    "\n",
    "    batch.append(vector)\n",
    "\n",
    "    # ‚¨ÜÔ∏è Upload in batches\n",
    "    if len(batch) == batch_size or i == len(chunks) - 1:\n",
    "        index.upsert(vectors=batch)\n",
    "        batch = []  # Reset for next batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cf9d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check index statistics\n",
    "stats = index.describe_index_stats()\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d20617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: fetch first 3 vectors\n",
    "ids_to_fetch = [f\"chunk-{i}\" for i in range(3)]\n",
    "\n",
    "fetched = index.fetch(ids=ids_to_fetch)\n",
    "print(fetched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a422c0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "\n",
    "if not API_KEY:\n",
    "    print(\"No API_Key found, Please set the API_KEY.\")\n",
    "    exit(1)\n",
    "elif API_KEY.strip() != API_KEY:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb9a08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = OpenAI(api_key=API_KEY, base_url =BASE_URL)\n",
    "MODEL = \"llama3-70b-8192\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0a202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "\n",
    "model = SentenceTransformer(\"intfloat/e5-base-v2\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"You: \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    \n",
    "    query_vector = model.encode(f\"query: {query}\", normalize_embeddings=True).tolist()\n",
    "\n",
    "    results = index.query(\n",
    "        vector=query_vector,\n",
    "        top_k=10,  # Get top 5 most relevant chunks\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    context = \"\"\n",
    "    for match in results[\"matches\"]:\n",
    "        context += match[\"metadata\"][\"text\"].strip() + \"\\n---\\n\"\n",
    "\n",
    "    \n",
    "    # üîÅ Add memory to prompt\n",
    "    history_string = \"\"\n",
    "    for turn in chat_history[-3:]:  # Use last 3 Q&As (for brevity)\n",
    "        history_string += f\"User: {turn['question']}\\nAssistant: {turn['answer']}\\n\"\n",
    "\n",
    "    # Final prompt for the LLM\n",
    "    prompt = f\"\"\"\n",
    "    You are a highly accurate and honest assistant.\n",
    "    Use ONLY the context below to answer the question.\n",
    "    If the answer is not found in the context, say \"I don't know.\"\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question: {query}\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        streamed_response = groq_client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant who answers clearly and concisely.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            # stream=True   \n",
    "        )\n",
    "        answer = streamed_response.choices[0].message.content\n",
    "        # return answer\n",
    "\n",
    "        # Save to memory\n",
    "        chat_history.append({\"question\": query, \"answer\": answer})\n",
    "        \n",
    "        print(\"\\nü§ñ Bot:\", answer)\n",
    "\n",
    "        # result = \"\"\n",
    "        # for chunk in streamed_response:\n",
    "        #     content_piece = chunk.choices[0].delta.content or \"\"\n",
    "        #     result += content_piece\n",
    "        #     cleaned_result = result.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        #     yield cleaned_result  # <- Streaming to Gradio\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[LLM Error] {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_response(query: str, index, chat_history: list) -> str:\n",
    "#     \"\"\"Encode query, retrieve context, call LLM, and return answer.\"\"\"\n",
    "#     query_vec = model.encode(f\"query: {query}\", normalize_embeddings=True).tolist()\n",
    "#     results = index.query(vector=query_vec, top_k=5, include_metadata=True)\n",
    "#     context = \"\\n---\\n\".join(match[\"metadata\"][\"text\"] for match in results[\"matches\"])\n",
    "#     prompt = f\"Use the following context to answer the question:\\n\\nContext: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "#     try:\n",
    "#         resp = groq_client.chat.completions.create(\n",
    "#             model=MODEL,\n",
    "#             messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},{\"role\": \"user\", \"content\": prompt}]\n",
    "#         )\n",
    "#         answer = resp.choices[0].message.content.strip()\n",
    "#     except Exception as e:\n",
    "#         answer = f\"An error occurred: {e}\"\n",
    "#     chat_history.append({\"question\": query, \"answer\": answer})\n",
    "#     return answer\n",
    "\n",
    "# # Usage:\n",
    "# chat_history = []\n",
    "# while True:\n",
    "#     query = input(\"You: \")\n",
    "#     if query.lower() == \"exit\":\n",
    "#         break\n",
    "#     answer = get_response(query, index, chat_history)\n",
    "#     print(\"ü§ñ Bot:\", answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
