{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fee6519",
      "metadata": {
        "id": "3fee6519"
      },
      "source": [
        "# üß† Web Content Summarizer\n",
        "\n",
        "This notebook extracts and summarizes text content from websites using groq_client-compatible APIs (Groq or Ollama).\n",
        "\n",
        "## Features:\n",
        "- Scrapes website content (html, js, etc.. ) using `Selenium`\n",
        "- Uses groq_client-compatible models like `GROQ` & Local models like `Ollama` to generate summaries\n",
        "- Supports custom URLs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21d1066a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install requests python-dotenv beautifulsoup4 ipython groq_client selenium webdriver-manager gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "96c0d65d",
      "metadata": {
        "id": "96c0d65d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import socket\n",
        "import ipaddress\n",
        "import gradio as gr\n",
        "from dotenv import load_dotenv\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from IPython.display import display, Markdown, update_display\n",
        "from openai import OpenAI\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from webdriver_manager.chrome import ChromeDriverManager"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6c662251",
      "metadata": {
        "id": "6c662251",
        "outputId": "4990a898-b328-49ea-ce07-f96dc986ac2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key found and looks good so far!\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "BASE_URL = \"https://api.groq.com/openai/v1\"\n",
        "\n",
        "if not API_KEY:\n",
        "    print(\"No API_Key found, Please set the API_KEY.\")\n",
        "    exit(1)\n",
        "elif API_KEY.strip() != API_KEY:\n",
        "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c10a67b0",
      "metadata": {
        "id": "c10a67b0"
      },
      "outputs": [],
      "source": [
        "groq_client = OpenAI(api_key=API_KEY, base_url =BASE_URL)\n",
        "MODEL = \"llama3-70b-8192\"\n",
        "# ollama_with_openai = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe8d0bcd",
      "metadata": {
        "id": "fe8d0bcd"
      },
      "source": [
        "## üåê 2. Website Content Extractor\n",
        "This class retrieves the  following things from th website using `requests` or Selenium:\n",
        "`- Website Title` ,\n",
        "`- Website Content` ,\n",
        "`- Website Images` ,\n",
        "`- Website Videos` ,\n",
        "`- Website Links` ,\n",
        "and can be added more..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d1bb5128",
      "metadata": {
        "id": "d1bb5128"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Explanation of this code: https://chatgpt.com/share/686f8aed-a210-8007-970d-37906975fa4f\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def is_safe_url(url):\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        if parsed.scheme not in [\"http\", \"https\"] or parsed.netloc == \"\":\n",
        "            return False\n",
        "\n",
        "        host = parsed.hostname\n",
        "        ip = ipaddress.ip_address(socket.gethostbyname(host))\n",
        "        if ip.is_private or ip.is_loopback or ip.is_reserved or ip.is_link_local:\n",
        "            return False\n",
        "    except Exception:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "class WebScraper:\n",
        "    \"\"\"\n",
        "    A utility class to represent a Website that we have scraped, using Selenium, with extracted links.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, url):\n",
        "        if not is_safe_url(url):\n",
        "            raise ValueError(\"Invalid or unsafe URL\")\n",
        "\n",
        "        self.url = url\n",
        "        self.title = \"No title found\"\n",
        "        self.text = \"\"\n",
        "        self.links = []\n",
        "        self.images = []\n",
        "        self.files = []\n",
        "        self.tables = []\n",
        "\n",
        "        # Setup Selenium\n",
        "        options = Options()\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--disable-gpu\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--disable-extensions\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "\n",
        "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "        try:\n",
        "            driver.set_page_load_timeout(60)\n",
        "            driver.get(url)\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "            # Get title\n",
        "            self.title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "\n",
        "            # Remove irrelevant tags\n",
        "            if soup.body:\n",
        "                for tag in soup.body([\"script\", \"style\", \"input\"]):\n",
        "                    tag.decompose()\n",
        "                self.text = soup.body.get_text(strip=True)\n",
        "\n",
        "            # Extract all Images\n",
        "            all_images = [img.get(\"src\") for img in soup.find_all(\"img\") if img.get(\"src\")]\n",
        "            self.images = all_images\n",
        "\n",
        "            # Extract all valid links\n",
        "            all_links = [a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\") and is_safe_url(a.get(\"href\"))]\n",
        "            self.links = all_links\n",
        "\n",
        "            # Extract all tables\n",
        "            all_tables = [table for table in soup.find_all(\"table\")]\n",
        "            self.tables = all_tables\n",
        "\n",
        "\n",
        "        except WebDriverException as e:\n",
        "            print(f\"Error loading page with Selenium: {e}\")\n",
        "        finally:\n",
        "            driver.quit()\n",
        "\n",
        "    def get_contents(self):\n",
        "        return f\"-> Webpage Title:\\n{self.title}\\n\\n\\n-> Webpage Contents (limited text displayed up to 1000 characters):\\n{self.text[:1000]}\\n\\n\\n-> Links (limited to 20 links displayed):\\n{self.links[:20]}\\n\\n\\n-> Images:\\n{self.images}\\n\\n\\n-> Tables:\\n{self.tables}\\n\\n\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48d143f4",
      "metadata": {
        "id": "48d143f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-> Webpage Title:\n",
            "Full stack Development Company - Microweb Software Pvt Ltd\n",
            "\n",
            "\n",
            "-> Webpage Contents (limited text displayed up to 1000 characters):\n",
            "HomeAbout MicrowebCasesServicesTechnologyTechnology SubCloud Application Development ServicesAzure DevOps ServicesAI & ML ServicesShopify DevelopmentGolang Development ServicesDevOps Consulting ServicesWebflow DevelopmentBusiness TransformationLaravel Application DevelopmentSymfony Web DevelopmentNode.js DevelopmentAngularJs Web Development ServicesRuby on Rails Application DevelopmentMicrosoft DevelopmentMobile Application DevelopmentIoT and Embedded Systems & Smart SolutionsCloud TechnologyReactJS DevelopmentDrupal Web Development Services2D and 3D Video AnimationUI & UX DesignWeb DevelopmentEnterprise SolutionsDigital MarketingSoftware Outsource to IndiaGraphic DesigningeCommerce DevelopmentWordPress DevelopmentWooCommerce DevelopmentShopify DevelopmentPython DevelopmentGet in touchGet in touchWe Build Brilliance!WhoMicroweb software specializes in translating digital challenges into seamless strategies. Strategies that deliver!We make sites, apps and digital platforms. The end user\n",
            "\n",
            "\n",
            "-> Links (limited to 20 links displayed):\n",
            "['https://www.microwebtec.com', 'https://www.microwebtec.com/', 'https://www.microwebtec.com/about/', 'https://www.microwebtec.com/cases/', 'https://www.microwebtec.com/services/', 'https://www.microwebtec.com/technologies/cloud-application-development-services/', 'https://www.microwebtec.com/technologies/azure-devops-services/', 'https://www.microwebtec.com/technologies/ai-ml-services/', 'https://www.microwebtec.com/technologies/shopify-development/', 'https://www.microwebtec.com/technologies/golang-development-services/', 'https://www.microwebtec.com/technologies/devops-consulting-services/', 'https://www.microwebtec.com/technologies/webflow-development/', 'https://www.microwebtec.com/business-transformation/', 'https://www.microwebtec.com/technologies/laravel-application-development/', 'https://www.microwebtec.com/technologies/symfony-web-development/', 'https://www.microwebtec.com/technologies/node-js-development/', 'https://www.microwebtec.com/technologies/angularjs-web-development-services/', 'https://www.microwebtec.com/technologies/ruby-on-rails-development/', 'https://www.microwebtec.com/technologies/microsoft-development/', 'https://www.microwebtec.com/technologies/mobile-application-development/']\n",
            "\n",
            "\n",
            "-> Images:\n",
            "['https://www.microwebtec.com/wp-content/uploads/2023/11/logo.png', 'https://www.microwebtec.com/wp-content/uploads/2024/12/MW-Web-Banner-4.jpg', 'https://www.microwebtec.com/wp-content/uploads/2024/12/MW-Web-Banner-5.jpg', 'https://www.microwebtec.com/wp-content/uploads/2024/12/MW-Web-Banner-3.jpg', 'https://www.microwebtec.com/wp-content/uploads/2023/11/logo.png', 'https://www.microwebtec.com/wp-content/uploads/2023/12/laravel_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/symfony_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/drupal_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/wordpress_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/angularjs_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/nodejs_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/vuejs_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/adobe-illustrator_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/adobe-photoshop_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/figma_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/python_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/rails_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/microsoft_365_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/12/dotnet_f.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/11/Linkedin.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/11/Facebook.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/11/Twitter.svg', 'https://www.microwebtec.com/wp-content/uploads/2023/11/Instagram.svg', 'https://www.microwebtec.com/wp-content/uploads/2025/06/proud-bni-logo.jpg', 'https://www.microwebtec.com/wp-content/uploads/2024/11/Ahmedabad-city-e1732692367730.png', 'https://www.microwebtec.com/wp-content/themes/microweb/images/arrow_black.png', 'https://www.microwebtec.com/wp-content/themes/microweb/images/arrow_white.png', 'https://www.microwebtec.com/wp-content/themes/microweb/images/grey-logo-loader.png', 'https://www.microwebtec.com/wp-content/themes/microweb/images/logo-loader.png', 'https://www.microwebtec.com/wp-content/themes/microweb/images/logo_black.png', 'https://www.microwebtec.com/wp-content/uploads/wpcf7_captcha/52067773.png']\n",
            "\n",
            "\n",
            "-> Tables:\n",
            "[]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "website = WebScraper(\"https://www.microwebtec.com/\")\n",
        "# print(website.get_contents())\n",
        "print(website.get_contents())\n",
        "# website.links\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59316142",
      "metadata": {
        "id": "59316142"
      },
      "source": [
        "## üìù 3. Prompt Engineering and Summarization\n",
        "This section defines the prompts used to instruct the `large language model (LLM)` for summarization and provides the core functions to perform the website summarization using the `selected LLM (Groq or Ollama)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "64406c06",
      "metadata": {
        "id": "64406c06"
      },
      "outputs": [],
      "source": [
        "system_prompt = (\n",
        "    \"You are a highly intelligent assistant tasked with analyzing website content. \"\n",
        "    \"Your job is to extract and summarize the **core purpose** and **main content** of the site, ignoring any navigation bars, footers, cookie banners, or repetitive UI elements. \"\n",
        "    \"You excel at identifying meaningful information such as services offered, articles, announcements, product details, or business descriptions. \"\n",
        "    \"Always format your response in **markdown**, and present the summary in a clean, human-readable format that would make sense to someone who has never seen the site before.\"\n",
        ")\n",
        "\n",
        "\n",
        "def user_prompt_for(website):\n",
        "    user_prompt = (\n",
        "        f\"## Website Title\\n\"\n",
        "        f\"**{website.title}**\\n\\n\"\n",
        "        f\"## Instructions\\n\"\n",
        "        f\"You are analyzing the contents of this website. Please provide a clear and concise markdown summary of the website‚Äôs purpose and content. \"\n",
        "        f\"Include any relevant details such as:\\n\"\n",
        "        f\"- Services, products, or features offered\\n\"\n",
        "        f\"- Blog posts, articles, or resources\\n\"\n",
        "        f\"- News or announcements\\n\"\n",
        "        f\"- Contact information, locations, or teams (if present)\\n\\n\"\n",
        "        f\"### Important:\\n\"\n",
        "        f\"- **Ignore** navigation links, UI controls, cookie notices, or footer text.\\n\"\n",
        "        f\"- Focus on meaningful, unique content visible to a visitor.\\n\\n\"\n",
        "        f\"## Website Content\\n\"\n",
        "        f\"{website.text[:5000]}\"\n",
        "    )\n",
        "    return user_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbba4ea2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def summarize(url):\n",
        "    try:\n",
        "        website = WebScraper(url)\n",
        "        streamed_response = groq_client.chat.completions.create(\n",
        "            model=MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
        "            ],\n",
        "            stream=True\n",
        "        )\n",
        "        # return display(Makrdown(streamed_response.choices[0].message.content))\n",
        "\n",
        "        result = \"\"\n",
        "        for chunk in streamed_response:\n",
        "            content_piece = chunk.choices[0].delta.content or \"\"\n",
        "            result += content_piece\n",
        "            cleaned_result = result.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
        "            yield cleaned_result  # <- Streaming to Gradio\n",
        "\n",
        "    except Exception as e:\n",
        "        yield f\"[LLM Error] {e}\"  # <- yield for Gradio to handle the stream\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "af62785a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# summarize(\"https://www.microwebtec.com/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b0dc320b",
      "metadata": {
        "id": "b0dc320b",
        "outputId": "be9740d2-4cfe-447b-b4f8-475e5785fd98"
      },
      "outputs": [],
      "source": [
        "# summarize(\"https://www.gopalinfo.com/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0d7a6475",
      "metadata": {
        "id": "0d7a6475",
        "outputId": "a0bfa89f-e503-4648-9e13-3a4ad06473d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://fdd3c31a9103bcb37f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://fdd3c31a9103bcb37f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "view = gr.Interface(\n",
        "    fn=summarize,\n",
        "    inputs=gr.Textbox(label=\"Enter Website URL\"),\n",
        "    outputs=gr.Markdown(label=\"Summary\"),\n",
        "    title=\"Website Summarizer\",\n",
        "    description=\"Paste a URL and get a clean summary of the site's purpose and content.\",\n",
        "    flagging_mode=\"never\"\n",
        ")\n",
        "\n",
        "view.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
