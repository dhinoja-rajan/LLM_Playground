{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3fee6519",
      "metadata": {
        "id": "3fee6519"
      },
      "source": [
        "# üß† Web Content Summarizer\n",
        "\n",
        "This notebook extracts and summarizes text content from websites using OpenAI-compatible APIs (Groq or Ollama).\n",
        "\n",
        "## Features:\n",
        "- Scrapes content using `requests` or `Selenium`\n",
        "- Uses OpenAI-compatible models like `grok` & Local models like `Ollama` to generate summaries\n",
        "- Supports custom URLs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "96c0d65d",
      "metadata": {
        "id": "96c0d65d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import json\n",
        "import socket\n",
        "import ipaddress\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse\n",
        "from IPython.display import display, Markdown, update_display\n",
        "from openai import OpenAI\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.common.exceptions import WebDriverException\n",
        "from webdriver_manager.chrome import ChromeDriverManager"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17dbc0f1",
      "metadata": {
        "id": "17dbc0f1"
      },
      "source": [
        "## üîß 1. Load API Keys\n",
        "We load the API key from a `.env` file to securely connect to the Groq or Ollama LLM backend.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "6c662251",
      "metadata": {
        "id": "6c662251",
        "outputId": "4990a898-b328-49ea-ce07-f96dc986ac2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key found and looks good so far!\n"
          ]
        }
      ],
      "source": [
        "load_dotenv(override=True)\n",
        "api_key  = os.getenv(\"GROQ_API_KEY\")\n",
        "\n",
        "if not api_key:\n",
        "    print(\"No API_Key found, Please set the API_KEY.\")\n",
        "    exit(1)\n",
        "elif api_key.strip() != api_key:\n",
        "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
        "else:\n",
        "    print(\"API key found and looks good so far!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c10a67b0",
      "metadata": {
        "id": "c10a67b0"
      },
      "outputs": [],
      "source": [
        "openai = OpenAI(api_key=api_key, base_url = \"https://api.groq.com/openai/v1\")\n",
        "ollama_with_openai = OpenAI(api_key = \"ollama\", base_url = \"http://localhost:11434/v1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe8d0bcd",
      "metadata": {
        "id": "fe8d0bcd"
      },
      "source": [
        "## üåê 2. Website Content Extractor\n",
        "This class retrieves the raw HTML and title of a webpage using `requests` or Selenium.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d1bb5128",
      "metadata": {
        "id": "d1bb5128"
      },
      "outputs": [],
      "source": [
        "def is_safe_url(url):\n",
        "    try:\n",
        "        parsed = urlparse(url)\n",
        "        if parsed.scheme not in [\"http\", \"https\"] or parsed.netloc == \"\":\n",
        "            return False\n",
        "\n",
        "        host = parsed.hostname\n",
        "        ip = ipaddress.ip_address(socket.gethostbyname(host))\n",
        "        if ip.is_private or ip.is_loopback or ip.is_reserved or ip.is_link_local:\n",
        "            return False\n",
        "    except Exception:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "class Web_Scapper:\n",
        "    \"\"\"\n",
        "    A utility class to represent a Website that we have scraped, using Selenium, with extracted links.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, url):\n",
        "        if not is_safe_url(url):\n",
        "            raise ValueError(\"Invalid or unsafe URL\")\n",
        "\n",
        "        self.url = url\n",
        "        self.title = \"No title found\"\n",
        "        self.text = \"\"\n",
        "        self.links = []\n",
        "\n",
        "        # Setup Selenium\n",
        "        options = Options()\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--disable-gpu\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--disable-extensions\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "\n",
        "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
        "\n",
        "        try:\n",
        "            driver.set_page_load_timeout(20)\n",
        "            driver.get(url)\n",
        "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
        "\n",
        "            # Get title\n",
        "            self.title = soup.title.string.strip() if soup.title else \"No title found\"\n",
        "\n",
        "            # Remove irrelevant tags\n",
        "            if soup.body:\n",
        "                for tag in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
        "                    tag.decompose()\n",
        "                self.text = soup.body.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "            # Extract all valid links\n",
        "            all_links = [a.get(\"href\") for a in soup.find_all(\"a\") if a.get(\"href\")]\n",
        "            self.links = all_links\n",
        "\n",
        "        except WebDriverException as e:\n",
        "            print(f\"Error loading page with Selenium: {e}\")\n",
        "        finally:\n",
        "            driver.quit()\n",
        "\n",
        "    def get_contents(self):\n",
        "        return f\"Webpage Title:\\n{self.title}\\n\\nWebpage Contents:\\n{self.text}\\n\\n\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "48d143f4",
      "metadata": {
        "id": "48d143f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Webpage Title:\n",
            "Home - Edward Donner\n",
            "\n",
            "Webpage Contents:\n",
            "Skip to content\n",
            "Home\n",
            "Connect Four\n",
            "Outsmart\n",
            "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
            "About\n",
            "Posts\n",
            "Well, hi there.\n",
            "I‚Äôm Ed. I like writing code and experimenting with LLMs, and hopefully you‚Äôre here because you do too. I also enjoy DJing (but I‚Äôm badly out of practice), amateur electronic music production (\n",
            "very\n",
            "amateur) and losing myself in\n",
            "Hacker News\n",
            ", nodding my head sagely to things I only half understand.\n",
            "I‚Äôm the co-founder and CTO of\n",
            "Nebula.io\n",
            ". We‚Äôre applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I‚Äôm previously the founder and CEO of AI startup untapt,\n",
            "acquired in 2021\n",
            ".\n",
            "We work with groundbreaking, proprietary LLMs verticalized for talent, we‚Äôve\n",
            "patented\n",
            "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
            "Connect\n",
            "with me for more!\n",
            "May 28, 2025\n",
            "Connecting my courses ‚Äì become an LLM expert and leader\n",
            "May 18, 2025\n",
            "2025 AI Executive Briefing\n",
            "April 21, 2025\n",
            "The Complete Agentic AI Engineering Course\n",
            "January 23, 2025\n",
            "LLM Workshop ‚Äì Hands-on with Agents ‚Äì resources\n",
            "Navigation\n",
            "Home\n",
            "Connect Four\n",
            "Outsmart\n",
            "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
            "About\n",
            "Posts\n",
            "Get in touch\n",
            "ed [at] edwarddonner [dot] com\n",
            "www.edwarddonner.com\n",
            "Follow me\n",
            "LinkedIn\n",
            "Twitter\n",
            "Facebook\n",
            "Subscribe to newsletter\n",
            "Type your email‚Ä¶\n",
            "Subscribe\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "website = Web_Scapper(\"https://edwarddonner.com\")\n",
        "# print(web_scrapper.get_contents())\n",
        "print(website.get_contents())\n",
        "# web_scrapper.links\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59316142",
      "metadata": {
        "id": "59316142"
      },
      "source": [
        "## üìù 3. Prompt Engineering and Summarization\n",
        "This section defines the prompts used to instruct the `large language model (LLM)` for summarization and provides the core functions to perform the website summarization using the `selected LLM (Groq or Ollama)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "64406c06",
      "metadata": {
        "id": "64406c06"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"You are an assistant that analyzes the contents of a website and provides a short summary, ignoring text that might be navigation related. Respond in markdown.\"\n",
        "\n",
        "def user_prompt_for(website):\n",
        "    user_prompt = f\"- You are looking at a website titled: '{website.title}'.\\n\\n- The contents of this website is as follows; please provide a short summary of this website in markdown. If it includes news or announcements, then summarize these too.\\n{website.text}\"\n",
        "    return user_prompt\n",
        "# print(user_prompt_for(content))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "bf931267",
      "metadata": {
        "id": "bf931267"
      },
      "outputs": [],
      "source": [
        "# And now: call the OpenAI API. You will get very familiar with this!\n",
        "def summarize(url):\n",
        "    website = Web_Scapper(url)\n",
        "    response = openai.chat.completions.create(\n",
        "        model = \"llama3-70b-8192\",\n",
        "        messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt}\n",
        "        ,{\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
        "    ]\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "\n",
        "# A function to display this nicely in the Jupyter output, using markdown\n",
        "def display_summary(url):\n",
        "    summary = summarize(url)\n",
        "    display(Markdown(summary))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "b0dc320b",
      "metadata": {
        "id": "b0dc320b",
        "outputId": "be9740d2-4cfe-447b-b4f8-475e5785fd98"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Gopal Info Website Summary**\n",
              "================================\n",
              "\n",
              "Gopal Info is a digital growth agency that offers a range of services to help businesses thrive online. Their expert solutions include:\n",
              "\n",
              "* **Digital Design**: Graphic Designing, UI/UX Design, Branding Consultation, and Web Designing\n",
              "* **Development**: Web Development and Mobile App Development\n",
              "* **Marketing**: Social Media Marketing, Search Engine Optimization, and Google Ads\n",
              "\n",
              "The agency utilizes advanced technologies to deliver high-quality services and follows an agile methodology to ensure project success. They prioritize collaboration, creativity, and quality at every step.\n",
              "\n",
              "**No notable news or announcements on the website.**"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_summary(\"https://www.gopalinfo.com/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0d7a6475",
      "metadata": {
        "id": "0d7a6475",
        "outputId": "a0bfa89f-e503-4648-9e13-3a4ad06473d9"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**TechCrunch Website Summary**\n",
              "===========================\n",
              "\n",
              "TechCrunch is a website that provides news, articles, and insights on the tech industry, focusing on startups, venture capital, and technology. The website features various sections, including:\n",
              "\n",
              "### Latest News\n",
              "\n",
              "* Cluely's Roy Lee discusses cheating detectors\n",
              "* Microsoft shares $500M in AI savings internally\n",
              "* California lawmaker pushes for mandated AI safety reports\n",
              "* Neobank Revolut seeks $65B valuation\n",
              "* YouTube prepares crackdown on 'mass-produced' and 'repetitive' videos\n",
              "\n",
              "### Topics\n",
              "\n",
              "* AI\n",
              "* Startups\n",
              "* Venture\n",
              "* Apple\n",
              "* Security\n",
              "* Events\n",
              "* Podcasts\n",
              "* Newsletters\n",
              "\n",
              "### Upcoming Events\n",
              "\n",
              "* TechCrunch All Stage 2025 (July 15, Boston)\n",
              "* TechCrunch Disrupt 2025 (October 27-29, San Francisco)\n",
              "* StrictlyVC Palo Alto (December 3, Palo Alto)\n",
              "\n",
              "### Newsletters\n",
              "\n",
              "* TechCrunch Daily News\n",
              "* Startups Weekly\n",
              "* TechCrunch Week in Review\n",
              "* TechCrunch Mobility\n",
              "\n",
              "### Video\n",
              "\n",
              "* Interviews and talks on various topics, including crypto, hardware, startups, and AI\n",
              "\n",
              "Overall, TechCrunch provides a comprehensive platform for staying updated on the latest developments in the tech industry, with a focus on innovation, entrepreneurship, and investment."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display_summary(\"https://techcrunch.com\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
